translate the following text for me to German but keep the markdown syntax

## part 1

The first thing is, this is not a basic TDD talk, I won't explain how to do TDD, right. So I would expect you to understand at least that TDD is a process by which we write tests before we write code. And there is a cycle of red to green and refactor, where we write a failing test, we then get that test to pass. And then we do any refactoring to improve the quality of our code. You need to know kind of that much. It doesn't matter if you're practicing TDD currently, it just matters that you understand kind of what TDD is about and how it generally gets presented. The other thing about this talk is I will definitely be having a, I will be criticizing what is often seen as the prevailing approach to test driven development, I'll be criticizing a kind of approach based on unit tests, mocks everything else. And even gherkin based syntax style acceptance tests. If you are deeply wedded to those, and no one is ever going to persuade you that that is the wrong way of doing something, you may want to find somebody else to talk to you because that is what I'm going to try and do today. And if that's not something you can go, I don't want to hear any of that, right? No point in you being here. Right. If you have an open mind, we'll try and explain different ways of doing things. And if you've tried TDD and you have bounced off it, this may help you understand why, and may help you understand a way that may work for you better. Hopefully, you will be able to write less simpler, better tests. This is not a code based talk as well, right? This is a theory based talk it tells explains the theory of how you do TDD correctly. So if you want something demo heavy, that isn't this talk either. Alright, so anyone who wants to run away, I am really happy that you go and find a talk that will work for you, rather than sort of sinking through my talk and being disappointed because it didn't give you what you wanted, right? Your time. I don't want to waste it. All right. Most of this is dull, identical. At that point at the bottom. There is often in lecture style relationships, a bit of a impression that I am somehow, you know, the experts in you are not. We're all just software engineers, I just got lucky enough, early on in my career, to have to go and talk at the user group I was running, we went to share talking because there were no experts that we could draw on in the early days of dotnet. So I got past the confidence issue of standing up and talking to you all, but I'm really smarter than any of you right? And you should you could do this, you just need to get audiences like user groups and stuff locally, where you can build up the confidence to talk in front of people. dotnet developers in the room, I work on an open source project for messaging called brighter and doing CTRs. We are very friendly towards open source contribution. other open source and commercial open source messaging frameworks do exist, we're not better or worse, but give us a try.
Alright, what's the agenda? There's not really an agenda, actually clean architecture is not on there anymore. And I think we're just gonna talk about the fallacies of TDD and the principles. Okay. The first and perhaps the most significant fallacy of test driven development is that developers write unit tests. Nothing could be further from the truth. TDD has nothing zero to do with unit tests. So if I look on Wikipedia, I can get a reasonable definition of unit testing, right. Wikipedia says to isolate issues that may arise. Each test case should be tested independently, substitutes, such as method stubs, mock objects, fakes, and test harnesses can be used to assist testing a module in isolation. What does that mean? Right? So once upon a time, when we talked about how we were going to automate testing, we tested modules, modules nebulously defined, right? It could be a class, it could be much bigger. And the idea was that module is treated as a black box, and your test probe of the module from the outside. And you wanted to make sure that any failures your testing gave you were inside the box. So any dependencies that box Have you would replace with a fake or substitute that's classical automated testing. Okay. The problem is trying to apply this as a paradigm to test driven development. So what happened was when test driven development merged, some people who were familiar with automated testing, assumed and began to teach other people TDD on the basis that it was just automated testing, and it's not. So there are some problems with that, right? The first problem is there's a lot of focus on isolation in TDD, right. The idea is that for defect localization, I need to understand that all the errors are in this component, not one of its collaborators. Therefore, I must use substitutes that I understand for all of its collaborators. Test doubles are a term we tend to use to describe these things. The idea is about a stunt double, right, something that stands in for the actor who we can't expose to danger.

## part 2 
translate the following text for me to German. 

And so that what happens in a lot of people who are influenced by unit testing is they take a class, because they decide that's the equivalent of a module from automated software engineering testing. And they replaced all of its dependencies with substitutes. And eventually, what this led to was a paradigm of thought in test driven development, sometimes called need driven development, those of you have read the book Growing object oriented software with tests. This book espouses particularly this philosophy of what's called need driven development. It's a variation on the test driven development process where code is written from the outside in. So in other words, you start effectively at the outside with the kind of function or method that you're getting under test. And all dependent upon code is replaced by mock objects that verify the expected indirect outputs of the code being written. So in other words, if I have any kind of collaborator, then I'm going to replace it with a substitute. And I'm going to verify in my test, the calls I make to that substitute. One of the issues here is that generally, this involves upfront design, I have to either understand perhaps I did CRC cards, perhaps I had a white board, why how I'm partitioning my domain space into objects, because I have to know, well, this responsibility isn't the thing under test, it's the responsibility of something else, which I would effectively replaced with a mock or a stop. So I'm doing upfront design, I'm not letting the tests inform my design.

Another alternative to this, which is the one that growing object onto a software test tends to basically offer is the, as you begin to do your implementation in response to the test, you realize, oh, this next bit, probably needs to be another, another class and the responsibility needs to be handed off. So at that point, I will then I'm doing discovery. So I will now write my tests for that mock. Alright. There's an overall problem with this, right? That's basically began to hamper a lot of our development. And that is that essentially, we now in our test, understand the implementation details of the method or class that we are going to basically test, we must understand his details, because we have to understand the interaction it has with collaborators in order to get his job done. We also tend to get forced to interworld when most of those collaborators are public, things that we can otherwise test, they're not internal or private, they are not hidden. As long ago as 2007, which makes me feel really old. I wrote this because the TDD community understood at the time that people were going in the wrong direction. And we tried shouting a lot. But this automated software engineering kind of like paradigm had really took taken over. And back in then I said, when I look around now I see a lot of people using mocks to replace all of their dependencies. My concern is they will begin to hit the fragile test issues that mocks present Gerrard mess as arrows identifies the issues we hit as to specific smells, over specified software and behavior sensitivity. So typically, what happens is in a strongly typed language like Java or C sharp, you say, Oh, my collaborators, I need to better substitute those, I will create an interface for them. So rather than doing anything up in the constructor of my class, if I want to basically use components so I can effectively compose behaviors rather than using inheritance, I have to inject to my dependencies. Previously, we tended to use dependency injection for things like strategy pan, or factory support layering where effectively I couldn't know basically about the concrete class and a layer that was above me, for example, but suddenly, we were using interfaces to inject all of our dependencies. And the result of that was we couldn't any, you will more use any kind of like, what we call poor man's di, where composition was from the roots and everything just, well, I can just new up my dependencies, etc, and Parsons from config. 

# part 3

## Developers ended up creating long chains of interfaces in constructors, leading to a reliance on IOC frameworks to manage code bases.
Suddenly we had these really long chains have interfaces in our constructors, which meant that we needed to start using IOC frameworks to build our code bases. If you ever wondered, hang on a minute, my class seems to be this massive dependencies, I can't actually create anything without IOC frameworks. 
## Question arises if relying heavily on IOC frameworks due to massive dependencies is beneficial, with the suggested answer being negative.
Is this the right thing? And the answer is probably no, we've kind of trapped ourselves, right? 
## Behavior sensitivity issue: Changes in implementation details should not break tests, which should focus on contracts rather than implementation specifics.
What I mean by behavior sensitivity, we mean. The problem is that essentially, we should be able to change the implementation details of our code without breaking tests. Write tests are intended to focus on the contract. But actually, our tests are focused on the implementation details. And generally what you see, you see this behavior when you try and change something, and then you go, I gotta fix to 300 Broken tests, because I changed the way we called that interface on the other class. Right? That shouldn't happen. 
## Common issue where minor changes in code result in numerous test failures, highlighting an over-focus on implementation details in tests.
You're changing an implementation detail that shouldn't break your tests. And that also leads to this problem where people stop refactoring. Or they try and work around basically avoiding making changes because it's kind of like, Yeah, I can't do that so many tests will break that will take us days to fix let's do something else. But tests were supposed to make our code more supple. 
## This over-focus on implementation details often discourages refactoring or leads developers to avoid changes altogether due to the high cost of fixing numerous tests.
They're supposed to enable refactoring, why are they preventing us from refactoring? 
## Tests are meant to enable refactoring and make code more adaptable, not to prevent these improvements.
So the principle you need to understand a principle that's been around since the beginning of TDD is the developers write developer tests, sometimes called programmer tests, not unit tests.
## The original intent of TDD is for developers to write developer tests (or programmer tests) which focus on the contract, not detailed unit tests.
 It's worth noting, so when Kent writes TDD by example, Kent is not the originator of TDD. 
## Kent Beck, though not the originator of TDD, documented existing practices in "TDD by Example," discussing the broad usage and understanding of TDD principles including mock and acceptance tests. 
 And he would admit to that, I think, Ken is documenting a practice that he and others in various software communities have been using for some time. And if you can read TDD by example, it has information on marks acceptance, test driven development, lots of ideas that people think came much later, they already knew about, they already understood and it's discussed, right. So he's documenting practice that people are using for a wider audience. 
## Misinterpretation and misuse of the term "unit tests" influenced by automated software engineering approaches led to a distortion of TDD practices. 
 And Kent says, I call them unit tests. But they don't match the accepted definition that was unit tests very well, this is the only reference to unit tests in the entire book, you can search it, that's the only time it's mentioned. What it means is in casual conversation, we occasionally say unit tests, when he built J unit, you refer to them as unit tests. It's one of these kind of like, you know, billion dollar mistakes, right. But like using the word micro with micro services, right? 

People started to get to go off on a tangent of completely the wrong set of ideas. And they brought in units from automated software engineering. Right? So we can understand what we should be doing better if we look at refactoring. 
## Martin Fowler defines refactoring as making internal changes to software to simplify and reduce costs of modification without altering observable behavior, which should guide TDD practices.
So this is Martin Fowler's definition of refactoring, a change made to the internal structure of software to make it easier to understand and cheaper to modify without changing its observable behavior. So refactoring is very clear, right? I've got some observable behavior, things that other people depend upon that my class effectively does, right? If I want to change how it does it, a more efficient algorithm, better structure, effectively breaking it into small classes, I should be able to do that. Right? Guided by my tests to ensure that effectively, I don't break anything. So red green refactor is the TDD cycle. refactor is the last step. 
## The TDD cycle (Red-Green-Refactor) often misses the 'refactor' phase, leading to accumulated design and code quality issues.
Often you see people practicing TDD going? Yeah. I mean, we just do a lot of red and green, we never really refactor. Right, that's a smell. It's a smell that effectively what's happened is you've got a design up front, you are now trying to get under test. Because in the green phase, what you should be doing is anything you can to find the algorithm that you need to pass the test. So you can go and copy code from Stack Overflow copypasta, that's absolutely fine. In the green phase, you can go and ask chant GPT to tell you how you should write that code and cut and paste it in. That's absolutely fine. 

## In the "green" phase of TDD, any means necessary may be used to pass the test, including copying code, with the expectation that the code will be refined during the refactor phase.
And the green phase, you will make the code good once it passes the test because now you know exactly the code you need to pass the test. And you can then refactor to write high quality code, right? You can improve on what chat GPT gave you So your code is exposing a contract. And what we want you to do is test that contract test, I can implement that contract, and then have tests which say I continue to meet that contract. But you should better change how you implement that contract, without breaking any of your tests. 
## The use of mocks often leads to brittle tests that break with any implementation change, contradicting the TDD principle of testing the contract rather than the implementation.
And the problem with mocks is that every time you tried to change the implementation details, you will keep breaking your tests came back said basically, I think somewhere in mid teens, basically, if a program's behavior is stable, from an observers perspective, no test to change other words when I'm refactoring, and I changed the contract, change the implementation rather than the contract. And I shouldn't change the tests. Right? So it's a contract first approach to testing in TDD, right. Behavior is the contract the API that you are exposing, in other words, the public methods of a class, right. 
## Kent Beck emphasized a contract-first approach in TDD, advocating for tests that are coupled to behavior and decoupled from structure, ensuring tests validate the functionality rather than the code structure itself.
And again, Kent said even more recently, because this whole thing, you know, we never seem to win against the bad ideas of automated software engineering. tests should be coupled to the behavior of code and decoupled from the structure of code, seeing tests that fail on both counts. In other words, the tests express the contract that you want to see from that particular class. Not how you've chosen to implement that. 
## A meeting with notable figures like DHH (creator of Ruby on Rails), Kent Beck, and Martin Fowler discussed the pitfalls of misapplying TDD principles, particularly how over-reliance on mocks and unit tests from automated software engineering inhibits effective TDD practices.
There was a big kind of meet up with DHH, greater Ruby on Rails enfant, cerebra of our industry, Kent Beck, Oxford, Martin Fowler, because DHH in his usual way to obtain clicks, effectively said, I'm done with TDD. And I feel good about saying that it's basically not done me any good, I end up with these test suites that make change in my code, heart and everything else, right. And the thing is, I think, to be fair, what he'd done is fallen into the very much the automated software engineering unit test based approach of actively testing. And Ken said in that interview, that the dialogue that Martin basically created between the two of them, my personal style, is I just don't go very far down the mock path, your test is completely coupled to the implementation of the interface. Of course, you can't change anything without breaking the tests.

# Part 4a (17:33-)
## TDD avoids mock-driven testing, focusing instead on direct state changes between preconditions and postconditions.
So we don't do this kind of mock driven testing, right? Because that means effectively we are observing the thing under test by indirect outputs, not by simply saying something along the lines of there is state before there is state after and I've somehow transformed it right preconditions and postconditions. That's really what you want to focus on right? preconditions and postconditions, what would the world look like before I did this? Exercise this behavior? What does the world look like now. So this is the definition of actually the unit test right? 
## Unit tests in TDD aim to localize defects to a single unit like a method or class, implicating only the most recent code edit if a failure occurs.
Failure, every unit test shall implicate one on one only unit, a method class or module or package. So unit tests do defect localization via substitution. This is the definition of a programmer or developer test that you use in test driven development. Failure of a program or develop a test under test driven development implicates only the most recent edit. 
So the way you do defect localization is I have a suite of green tests, I make a small change to my code. If the, if any tests now go red, which used to be green, the defect is localized to them to the code I just wrote. 
## Small, incremental coding changes are encouraged in TDD to facilitate easier defect localization and reduce the need for extensive debugging.
This is the reason in TDD, we recommend that you take small jumps, what's the smallest increment of behavior that you could now get under test? Because that localizes the defect. So Kent has a thing about driving in gears, which I think must confuse Americans, because I'm pretty sure they drive automatic rather than stick shift. But what he's talking about is this idea that the higher the gear you're wearing, the more code you write at any one time, the more confident you have to be about what you're doing, the more you have to feel I've done this 1000 times, it's okay, I can write 80 lines of code without there being too many defects I'm good, the less comfortable you feel. The smaller the amount of code, the narrower the test you need to write in order to essentially understand where the defects are. So I typically get questions about oh, when he talks about TDD in this in this way. 

## TDD's incremental approach allows for effective testing of larger systems, like HTTP APIs, by managing the scale of code changes.
Can I do it by TDD my HTTP API. And the answer is sure you could, if that works for you, but you're taking a big jump. So defect localization now is right across that span of your, your web controller, the marshaling basically, of stuff into format cetera, all the way down to your handler, if that's actually quite simple, maybe you're comfortable driving at a level where effectively you make those big jumps. And you're okay with figuring out where defects are in that, right? If not, you want to go a bit lower, right. So maybe you want to support adapters architecture and tested the poor. Or maybe it's a really complex algorithm, and you want to test lower down at the individual class. But it's kind of up to you to make a choice. TDD doesn't really have a rule other than driving a gear you feel comfortable with, that you can localize the defect in the amount of code you've just committed. And if you find yourself hitting the debugger a lot, you're making too big a jump. Right? Real, the hardcore TDD folks from back in early to late 90s, early 2000s used to say, 
## Traditional TDD advice emphasizes deleting large, problematic code segments to maintain manageable increments and avoid debugging reliance.
Oh, if you've added a whole if you added sort of 100 lines of code, and then you break stuff, just in test, break, delete that code and try again, right? And it's pretty hardcore. I'm okay, I'd be like, you can use a debugger, right. But the reason that I had that rule was to try and persuade people not to write 200 lines of code with each test, because you'd have to lose 200 lines of code. So that's the reason for the rule is trying to basically say to people don't jump too far. Okay. But a full definition of test driven development is the CT wiki, which is kind of slowly fading in the way that, you know, our history on the internet is steady getting killed, which is never a great thing says test driven development produces developer tests, the failure of a test case implicates only the developer's most recent Edit.
## The core principle of TDD is producing developer tests that focus on the latest changes without extensive use of mocks or debugging, maintaining a focus on minimal and precise code changes.
 This implies that developers don't need to use mock objects to split their code up into testable units. And it implies a developer may always avoid debugging by reverting that last edit, right. So I tend to basically in casual conversation where I can try and use programmer developer tests, even I will occasionally use the word unit right because become so prevalent. 

# Part 4b (-27:09)
## TDD is characterized by minimal upfront design, focusing instead on discovering the implementation gradually through a contract-driven approach.
TDD is a process of discovery, right? In other words, you are seeking to effectively not do huge amounts of design upfront, take your contract, and then effectively, slowly explore the implementation that you need. And so you don't use mock objects to the same extent you would in his classical model, you don't need to isolate the thing under test. 
## Unlike classical models, TDD does not rely heavily on mock objects, allowing for more natural use of new objects within constructors without dependency injection.
There are some cases and mock objects, we'll talk about them in a minute. And that means essentially, you can do stuff like use New to create a class in your a component in your constructor rather than inject it. It's amazing that liberating when you can say, hey, I don't actually need to use di for everything, right. There is one use case, where effectively, we may want to think about using substitutes. So the confusion around unit test actually really stem as most of all from this statement by Kent, how should the running of tests affect one another? Not at all. 
## Kent Beck emphasized that tests should run independently and in parallel to ensure speed and avoid disrupting the developer's workflow, using the term "unit" to describe this isolation.
So when he was using unit, His thing was, your tests should be able to run in parallel. tests need to be fast, because we don't want to break your flow, and force you out of basically the work you're doing in incrementally working towards solving a requirement by making you go into a hole making you go, I'll go and make coffee while my tests run, right? We don't want that we want test to be I press a button within 30 seconds, okay, that's all green, I can keep moving, right. Whereas if he's kind of like, I run my tests, they want fancier coffee, then that's a disaster. Because I break my flow, the model gets torn down, I have to start step things up again. The Kent's thing was isolation of tests from each other. And that's why he inadvertently use the word unit. And that's what caused all the problems a little bit write tests are isolated from each other, so that they can run in parallel, which keeps them fast. 

## Common issues in testing arise from shared fixtures, where tests alter the same state and affect each other's outcomes, leading to unpredictable results.
The most common problem that causes tests not being able to run together is what we call shared fixture. In other words, that means something we're effectively two tests will change the same set of state. So for example, you have a table in a database. And one thing says, I'm going to count the number of rows after I do this operation. Another test says, I'm going to add some rows and if you run those two, they're going to come run in a random order. And essentially, if the one that adds the other rows, runs basically at the wrong moment in time he could affect the results the other ones and he gets a ratty IQ test that fails occasion, right from the plumbing you might have seen that right? Or static. In you know languages like C sharp we use a static verb variables. So effectively you have global state, and you're basically impacting global state in your test. So, when you have a shared fixture, you need to avoid the problem with a shared fixture. One way to do that is to use basically a test app or like a mock or a fake, it isn't the only way. 

Right, you may be able to actually make sure that you protect that you may have to run know some of those tests sequentially, because you don't have too many of them. But one way to do that, so I O is most commonly the problem we experience for shared fixtures. 
## To mitigate shared fixture issues, mocks or fakes can be used, especially in I/O operations that are slow or involve external dependencies, to maintain test speed and independence.
And so quite, it's quite common to decide, I will replace this thing that's doing IO, like talking to a database with a shared with with a mock in order to avoid the shared fixture problem. The other reason we may want to use a mock for I O is just speed. Right? If this thing is slow, fragile, because it involves some kind of network connection or talks to resources that might not be available. In order to avoid the problem of test breaks, it's fragile or test take a long time. Again, it's another reason to replace something with a mock to TDD by example, Ken's back ticks all about mocks, but that's the reason they want to use them. Right? To allow your tests to run faster run independently not to isolate the thing under test. Okay. Let's dig a bit deeper. 
## TDD according to Wikipedia WRONGLY encourages writing tests for each new function, promoting a test-first approach that ensures each piece of functionality is properly defined and isolated before implementation.
The next thing I tend to see is that people get taught Okay, before you write any method, write a test for that method. So typically, the rule people have is the trigger for a new test is writing a new function. Even Wikipedia, bless its heart says write a test that defines a function or promote improvements of a function. Okay, Wikipedia is very wrong. Okay, it seems to make sense, right? We talked about basically saying tests while you have preconditions? How does the world look before I run my test? I have post conditions, how does the world look after I personally run my function? seems to make sense, that would be good interception point, right? Implementation is just how to implement that function.